{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\sentiment_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"./.pretrained_data/robertuito-sentiment-analysis\"\n",
    "DATA_PATH = \"./.data/task2_trainingData\"\n",
    "MAX_INPUT_LENGTH = 130 - 2\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from typing import Dict\n",
    "\n",
    "def preprocess_single_subject(filename, cut_point)->Dict:\n",
    "    \"\"\"Process a single subject json file\n",
    "\n",
    "    Args:\n",
    "        filename (str): target filename under directory DATA_PATH\n",
    "        cut_point (float): the percentage of used msg\n",
    "\n",
    "    Returns:\n",
    "        Tuple: a tuple with two element, the first is the input_ids, the second is the mask\n",
    "    \"\"\"\n",
    "    with open(os.path.join(DATA_PATH, filename), 'r', encoding=\"utf-8\") as fi:\n",
    "        raw_data = json.load(fi)\n",
    "    # sort message by date\n",
    "    message = []\n",
    "    for record in raw_data:\n",
    "        message.append((record['message'], datetime.datetime.strptime(record['date'], \"%Y-%m-%d %H:%M:%S\")))\n",
    "    message.sort(key=lambda x:x[1])\n",
    "    # cut off\n",
    "    message = message[:int(len(message)*cut_point)]\n",
    "\n",
    "    # extra the message text and get token\n",
    "    message = [tokenizer(record[0]) for record in message]\n",
    "\n",
    "    input_ids_res = []\n",
    "    attention_mask_res = []\n",
    "\n",
    "    input_ids_buffer = []\n",
    "    attention_mask_buffer = []\n",
    "\n",
    "    for msg_token in message:\n",
    "        tmp_input_ids = input_ids_buffer + msg_token['input_ids']\n",
    "        tmp_attention_mask = attention_mask_buffer + msg_token['attention_mask']\n",
    "        while len(tmp_input_ids) > MAX_INPUT_LENGTH:\n",
    "            input_ids_res.append(tmp_input_ids[:MAX_INPUT_LENGTH])\n",
    "            attention_mask_res.append(tmp_attention_mask[:MAX_INPUT_LENGTH])\n",
    "            tmp_input_ids = tmp_input_ids[MAX_INPUT_LENGTH:]\n",
    "            tmp_attention_mask = tmp_attention_mask[MAX_INPUT_LENGTH:]\n",
    "        if len(tmp_input_ids) > 0:\n",
    "            input_ids_buffer = tmp_input_ids\n",
    "            attention_mask_buffer = tmp_attention_mask\n",
    "        else:\n",
    "            input_ids_buffer = []\n",
    "            attention_mask_buffer = []\n",
    "    \n",
    "    if len(input_ids_buffer) > 0:\n",
    "        input_ids_res.append(input_ids_buffer)\n",
    "        attention_mask_res.append(attention_mask_buffer)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids_res,\n",
    "        'attention_mask': attention_mask_res\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n",
      "128\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "res = preprocess_single_subject(\"subject109.json\", 0.5)\n",
    "for i in res['input_ids']:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./.pretrained_data/robertuito-sentiment-analysis\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"NEU\",\n",
      "    \"2\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"NEU\": 1,\n",
      "    \"POS\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.29.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30002\n",
      "}\n",
      "\n",
      "loading weights file ./.pretrained_data/robertuito-sentiment-analysis\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./.pretrained_data/robertuito-sentiment-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaLayer\n",
    "from torch import nn\n",
    "\n",
    "def custom_robertlayer(origin):\n",
    "    origin.recurrent = nn.GRU(768, 768)\n",
    "    origin.recurrent_out = nn.Sequential(\n",
    "            nn.Linear(768, 768),\n",
    "            nn.LayerNorm(768, 1e-12, True),\n",
    "            nn.Dropout(0.1, False)\n",
    "    )\n",
    "    def customed_feed_forward_chunk(self, attention_output):\n",
    "        recurrent = self.recurrent(attention_output)[1]\n",
    "        recurrent = nn.Dropout(0.1, False)(recurrent)\n",
    "        recurrent_output = self.recurrent_out(recurrent)\n",
    "        intermediate_output = self.intermediate(recurrent_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "    \n",
    "    origin.__class__.feed_forward_chunk = customed_feed_forward_chunk\n",
    "\n",
    "    return origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    model.roberta.encoder.layer[i] = custom_robertlayer(model.roberta.encoder.layer[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 848, 852, 2726, 471, 1313, 588, 923, 669, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = tokenizer(\"Solo quiero entender un poco m√°s sobre esto\")\n",
    "print(len(test_sentence['input_ids']))\n",
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1125, -0.8250,  0.6899]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.tensor([test_sentence['input_ids']]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c05a16cb432961d4bf210c1384c5ca90b1aec85fe2b9c073fc80166a8b1e3b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
